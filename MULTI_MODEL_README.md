# Multi-Modell LLM Support

Das System unterst√ºtzt jetzt mehrere Open-Source-LLMs gleichzeitig mit intelligenter Modell-Auswahl.

## Features

- **Intelligentes Routing**: Automatische Auswahl des passenden Modells basierend auf Komplexit√§t
- **Parallel-Nutzung**: Mehrere Modelle k√∂nnen gleichzeitig verwendet werden
- **Austauschbarkeit**: Modelle k√∂nnen ohne Code-√Ñnderung ausgetauscht werden
- **Fallback-Strategien**: Automatisches Fallback bei Modell-Fehlern
- **Konfigurierbar**: Modelle √ºber Config-Datei oder Umgebungsvariablen

## Modell-Konfiguration

### √úber Config-Datei (empfohlen)

Erstellen Sie eine `llm_config.json` Datei im Projekt-Root:

```json
{
  "fast": {
    "model": "qwen2.5:3b",
    "fallback": "llama3.2:1b",
    "max_tokens": 150,
    "temperature": 0.1,
    "timeout": 10,
    "description": "Schnelles Modell f√ºr einfache Fragen"
  },
  "standard": {
    "model": "qwen2.5:7b",
    "fallback": "llama3.2:1b",
    "max_tokens": 400,
    "temperature": 0.2,
    "timeout": 30,
    "description": "Standard-Modell f√ºr normale Fragen"
  },
  "reasoning": {
    "model": "qwen2.5:32b",
    "fallback": "qwen2.5:7b",
    "max_tokens": 600,
    "temperature": 0.3,
    "timeout": 60,
    "description": "Reasoning-Modell f√ºr komplexe Fragen"
  }
}
```

### √úber Umgebungsvariablen

```bash
# Fast Modell
LLM_MODEL_FAST=qwen2.5:3b
LLM_FALLBACK_MODEL=llama3.2:1b

# Standard Modell
LLM_MODEL=qwen2.5:7b

# Reasoning Modell
LLM_MODEL_REASONING=qwen2.5:32b

# Router aktivieren/deaktivieren
USE_LLM_ROUTER=true

# Config-Datei (optional)
LLM_CONFIG_PATH=llm_config.json
```

## Routing-Logik

Das System w√§hlt automatisch das passende Modell basierend auf:

1. **Frage-L√§nge**: L√§ngere Fragen ‚Üí komplexeres Modell
2. **Reasoning-Keywords**: "warum", "weshalb", "erkl√§re" ‚Üí Reasoning-Modell
3. **Anzahl Kontext-Chunks**: Mehr Chunks ‚Üí komplexeres Modell
4. **Kontext-L√§nge**: L√§ngerer Kontext ‚Üí komplexeres Modell
5. **Relevance Score (RSQ)**: Niedrige Relevanz ‚Üí mehr Reasoning n√∂tig

### Komplexit√§ts-Schwellenwerte

- **< 0.3**: Fast Modell (qwen2.5:3b)
- **0.3 - 0.7**: Standard Modell (qwen2.5:7b)
- **> 0.7**: Reasoning Modell (qwen2.5:32b / Mixtral 8x22B / Qwen 72B)

## Unterst√ºtzte Modelle

### Aktuell getestet:
- **qwen2.5:3b** - Schnell, ressourcen-sparend
- **qwen2.5:7b** - Gute Balance
- **llama3.2:1b** - Sehr schnell, Fallback

### Geplant (ben√∂tigen mehr GPU-Speicher):
- **qwen2.5:14b** - Light Reasoning
- **qwen2.5:32b** - Strong Reasoning
- **qwen2.5:72b** - High-End Reasoning
- **mixtral:8x7b** - Mid-Reasoning
- **mixtral:8x22b** - High-End Reasoning

## Installation neuer Modelle

```bash
# Im Docker-Container
docker exec -it chatbotproject-ollama-1 ollama pull qwen2.5:32b

# Oder lokal
ollama pull qwen2.5:32b
```

## Aktivierung

Der Router ist standardm√§√üig aktiviert. Um ihn zu deaktivieren:

```bash
USE_LLM_ROUTER=false
```

## Monitoring

Die Logs zeigen, welches Modell verwendet wird:

```
üìä Komplexit√§t: 0.45 ‚Üí Modell: standard (qwen2.5:7b)
üß† Nutze Multi-Modell Router f√ºr intelligente Modell-Auswahl
```

## Flexibilit√§t

- **Hardware**: L√§uft auf jedem System mit Docker/Ollama
- **Deployment**: On-Prem, Bare Metal, Cloud - kein Code-Change n√∂tig
- **Skalierung**: Neue Modelle k√∂nnen einfach hinzugef√ºgt werden
- **Vendor-Lock-in**: Kein - alle Modelle sind Open-Source
- **Cloud-Zwang**: Kein - alles l√§uft lokal
